{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actividad 2 - Clasificación\n",
    "# Universidad Europea de Madrid\n",
    "Inteligencia Artificial<br>\n",
    "Isabel Sutil<br>\n",
    "M31<br>\n",
    "Jorge Javier Castilla Coello<br>\n",
    "Geronimo Basso Sosa<br>\n",
    "Luis Cabello Casquete<br>\n",
    "Paula Sáenz de Santa María Diez<br>\n",
    "Ana Esteban González<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo en común utilizado por todos los apartados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tabulate import tabulate\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 0\n",
    "Modificar el dataset para que cada vez que en \"prec\" se encuente el valor 0,0 se considere que no llueve, para los demas valores se va a considerar que si ha llovido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cargamos el fichero necesario para analizar los modelos.\n",
    "ruta_archivo = 'valores-climatologicos.csv'\n",
    "fdata = pd.read_table(ruta_archivo, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando una función lambda, para cada valor de ´prec´cambiamos su valor a 0 si la precipitación es 0\n",
    "cambiamos el valor a 1 si la precipitación es disitnto de 0. Además, creamos un mapeo de indivaticos de provincia para tener valores de tipo int en esa columna envez de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fdata['prec'] = fdata['prec'].apply(lambda x: '0' if x == 0.0 else '1') # 0 si no llovio, 1 si llovio.\n",
    "states = ['1387E', '1387', '1393', '1351', '1400', '1437O', '1473A', '1428', '1475X', '1505', '1484', '1484C', '1495']\n",
    "state_map = {state: idx for idx, state in enumerate(sorted(set(states)))}\n",
    "fdata['indicativo'] = fdata['indicativo'].apply(lambda x: state_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   indicativo  altitud  tmed prec  tmin  tmax  dir  velmedia  racha  presMax  \\\n",
      "0           2       98  16.9    0  12.0  21.8    1       3.1    8.3   1008.7   \n",
      "1           2       98  20.5    1  12.5  28.5   22       4.7   11.7   1008.5   \n",
      "2           2       98  21.4    0  17.6  25.1   22       2.2   10.3   1008.7   \n",
      "3           2       98  21.4    1  16.0  26.8   99       5.3   10.8   1009.4   \n",
      "4           2       98  18.6    0  15.7  21.6    4       3.9   11.7   1012.7   \n",
      "\n",
      "   presMin  \n",
      "0   1003.9  \n",
      "1   1005.5  \n",
      "2   1006.1  \n",
      "3   1005.4  \n",
      "4   1009.3  \n"
     ]
    }
   ],
   "source": [
    "# Visualización de las primeras filas del dataset.\n",
    "print(fdata.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 1\n",
    "Análisis exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Verificamos si hay valores nulos en nuestro dataset.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(fdata\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39many())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fdata' is not defined"
     ]
    }
   ],
   "source": [
    "# Verificamos si hay valores nulos en nuestro dataset.\n",
    "print(fdata.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indicativo       int64\n",
      "altitud          int64\n",
      "tmed           float64\n",
      "prec          category\n",
      "tmin           float64\n",
      "tmax           float64\n",
      "dir              int64\n",
      "velmedia       float64\n",
      "racha          float64\n",
      "presMax        float64\n",
      "presMin        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'prec' de 'object' a 'categorical'\n",
    "fdata['prec'] = pd.Categorical(fdata['prec'])\n",
    "print(fdata.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 2\n",
    "Análisis predictivo con Regresión Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indicativo     float64\n",
      "altitud        float64\n",
      "tmed           float64\n",
      "prec          category\n",
      "tmin           float64\n",
      "tmax           float64\n",
      "dir            float64\n",
      "velmedia       float64\n",
      "racha          float64\n",
      "presMax        float64\n",
      "presMin        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Crear un data set especifico con valores de tipo float para la regression lineal.\n",
    "floats_dataframe = fdata.astype(float)\n",
    "\n",
    "# Convertir la columna 'prec' de 'float' a 'categorical'\n",
    "floats_dataframe['prec'] = pd.Categorical(floats_dataframe['prec'])\n",
    "print(floats_dataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'floats_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m floats_dataframe\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprec\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Features\u001b[39;00m\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m floats_dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprec\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Variable de salida\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# División en 80/20 para entrenamiento y validación\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'floats_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "X = floats_dataframe.drop('prec', axis=1)  # Features\n",
    "y = floats_dataframe['prec']  # Variable de salida\n",
    "\n",
    "# División en 80/20 para entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Imprimir tamaños de los conjuntos\n",
    "print(\"Tamaño del conjunto de entrenamiento:\", len(X_train))\n",
    "print(\"Tamaño del conjunto de validación:\", len(X_val))\n",
    "\n",
    "# Crear conjuntos de entrenamiento y prueba\n",
    "conjunto_train = pd.concat([X_train, y_train], axis=1)\n",
    "conjunto_val = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Creamos dos conjuntos para poner las predicciones del train y test\n",
    "conjunto_train_eval = conjunto_train.copy()\n",
    "conjunto_validation_eval = conjunto_val.copy()\n",
    "\n",
    "# Método de control (k-fold cross-validation)\n",
    "metodo_control_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression Model trained in fold 1:\n",
      "Precisión en el step: 0.2897079771771981\n",
      "\n",
      "Linear Regression Model trained in fold 2:\n",
      "Precisión en el step: 0.28972270380803866\n",
      "\n",
      "Linear Regression Model trained in fold 3:\n",
      "Precisión en el step: 0.2897645807853185\n",
      "\n",
      "Linear Regression Model trained in fold 4:\n",
      "Precisión en el step: 0.28969591901494307\n",
      "\n",
      "Linear Regression Model trained in fold 5:\n",
      "Precisión en el step: 0.2897227144081307\n",
      "\n",
      "Linear Regression Model trained in fold 6:\n",
      "Precisión en el step: 0.2897278011997909\n",
      "\n",
      "Linear Regression Model trained in fold 7:\n",
      "Precisión en el step: 0.2896186774824374\n",
      "\n",
      "Linear Regression Model trained in fold 8:\n",
      "Precisión en el step: 0.2897401429317781\n",
      "\n",
      "Linear Regression Model trained in fold 9:\n",
      "Precisión en el step: 0.28967344783597193\n",
      "\n",
      "Linear Regression Model trained in fold 10:\n",
      "Precisión en el step: 0.28973398250702564\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo de Regresion Lineal.\n",
    "liner_regression_model = LinearRegression()\n",
    "\n",
    "# Crear un pipeline con escalado de características\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Escalar las características\n",
    "    ('model', liner_regression_model)        # Modelo de regresión logística\n",
    "])\n",
    "\n",
    "# Obtener todos los modelos de regresión lineal entrenados durante la validación cruzada\n",
    "liner_regression_models = [clone(pipeline).fit(X_train.iloc[train_idx], y_train.iloc[train_idx]) for train_idx, _ in metodo_control_cv.split(X_train, y_train)]\n",
    "\n",
    "# Imprimir información sobre el modelo\n",
    "for i, modelo in enumerate(liner_regression_models):\n",
    "    print(f\"\\nLinear Regression Model trained in fold {i + 1}:\")\n",
    "    precision = modelo.score(X_train, y_train)\n",
    "    print(\"Precisión en el step:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Apartado 3\n",
    "Análisis predictivo con KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el dataset del Apartado 2, dado que el mismo ya tiene sus valores convertidos como floats y necesitamos únicamente valores de tipo float o int en nuestro dataset para útilizar K-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K-NN Model trained in fold 1:\n",
      "Precisión en el step: 0.47782784069376016\n",
      "\n",
      "K-NN Model trained in fold 2:\n",
      "Precisión en el step: 0.4775850667089606\n",
      "\n",
      "K-NN Model trained in fold 3:\n",
      "Precisión en el step: 0.47858650939625913\n",
      "\n",
      "K-NN Model trained in fold 4:\n",
      "Precisión en el step: 0.48091020325077016\n",
      "\n",
      "K-NN Model trained in fold 5:\n",
      "Precisión en el step: 0.4795055823387151\n",
      "\n",
      "K-NN Model trained in fold 6:\n",
      "Precisión en el step: 0.4784737929033164\n",
      "\n",
      "K-NN Model trained in fold 7:\n",
      "Precisión en el step: 0.48040297903252804\n",
      "\n",
      "K-NN Model trained in fold 8:\n",
      "Precisión en el step: 0.4797136743256861\n",
      "\n",
      "K-NN Model trained in fold 9:\n",
      "Precisión en el step: 0.47812697292503126\n",
      "\n",
      "K-NN Model trained in fold 10:\n",
      "Precisión en el step: 0.4775330437122177\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo K-NN.\n",
    "k_nn_model = KNeighborsRegressor(n_neighbors=10)\n",
    "\n",
    "# Crear un pipeline con escalado de características\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Escalar las características\n",
    "    ('model', k_nn_model)        # Modelo de regresión logística\n",
    "])\n",
    "\n",
    "# Obtener todos los modelos de regresión lineal entrenados durante la validación cruzada\n",
    "k_nn_models = [clone(pipeline).fit(X_train.iloc[train_idx], y_train.iloc[train_idx]) for train_idx, _ in metodo_control_cv.split(X_train, y_train)]\n",
    "\n",
    "# Imprimir información sobre el modelo\n",
    "for i, modelo in enumerate(k_nn_models):\n",
    "    print(f\"\\nK-NN Model trained in fold {i + 1}:\")\n",
    "    precision = modelo.score(X_train, y_train)\n",
    "    print(\"Precisión en el step:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Apartado 4\n",
    "Análisis predictivo con un árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metodo_control_cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      5\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      6\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),      \u001b[38;5;66;03m# Escalar las características\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, decision_tree_model)    \u001b[38;5;66;03m# Modelo de Árbol de Decisión\u001b[39;00m\n\u001b[0;32m      8\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Obtener todos los modelos de Árbol de Decisión entrenados durante la validación cruzada\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m decision_tree_models \u001b[38;5;241m=\u001b[39m [clone(pipeline)\u001b[38;5;241m.\u001b[39mfit(X_train\u001b[38;5;241m.\u001b[39miloc[train_idx], y_train\u001b[38;5;241m.\u001b[39miloc[train_idx]) \u001b[38;5;28;01mfor\u001b[39;00m train_idx, _ \u001b[38;5;129;01min\u001b[39;00m metodo_control_cv\u001b[38;5;241m.\u001b[39msplit(X_train, y_train)]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Imprimir información sobre el modelo\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, modelo \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(decision_tree_models):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metodo_control_cv' is not defined"
     ]
    }
   ],
   "source": [
    "# Definir el modelo de Árbol de Decisión.\n",
    "decision_tree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Crear un pipeline con escalado de características\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Escalar las características\n",
    "    ('model', decision_tree_model)    # Modelo de Árbol de Decisión\n",
    "])\n",
    "\n",
    "# Obtener todos los modelos de Árbol de Decisión entrenados durante la validación cruzada\n",
    "decision_tree_models = [clone(pipeline).fit(X_train.iloc[train_idx], y_train.iloc[train_idx]) for train_idx, _ in metodo_control_cv.split(X_train, y_train)]\n",
    "\n",
    "# Imprimir información sobre el modelo\n",
    "for i, modelo in enumerate(decision_tree_models):\n",
    "    print(f\"\\nDecision Tree Model trained in fold {i + 1}:\")\n",
    "    precision = modelo.score(X_train, y_train)\n",
    "    print(\"Precisión en el step:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 5\n",
    "Análisis predictivo con SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo SVM para regresión.\n",
    "svm_model = SVR()\n",
    "\n",
    "# Crear un pipeline con escalado de características\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Escalar las características\n",
    "    ('model', svm_model)            # Modelo SVM para regresión\n",
    "])\n",
    "\n",
    "# Obtener todos los modelos SVM entrenados durante la validación cruzada\n",
    "svm_models = [clone(pipeline).fit(X_train.iloc[train_idx], y_train.iloc[train_idx]) for train_idx, _ in metodo_control_cv.split(X_train, y_train)]\n",
    "\n",
    "# Imprimir información sobre el modelo\n",
    "for i, modelo in enumerate(svm_models):\n",
    "    print(f\"\\nSVM Model trained in fold {i + 1}:\")\n",
    "    precision = modelo.score(X_train, y_train)\n",
    "    print(\"Precisión en el step:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apartado 6\n",
    "Análisis predictivo con Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo de Naive Bayes entrenado en el pliegue 1:\n",
      "Precisión: 0.6689423493714781\n",
      "\n",
      "Modelo de Naive Bayes entrenado en el pliegue 2:\n",
      "Precisión: 0.6728435197225835\n",
      "\n",
      "Modelo de Naive Bayes entrenado en el pliegue 3:\n",
      "Precisión: 0.6673168617251842\n",
      "\n",
      "Modelo de Naive Bayes entrenado en el pliegue 4:\n",
      "Precisión: 0.6686172518422193\n",
      "\n",
      "Modelo de Naive Bayes entrenado en el pliegue 5:\n",
      "Precisión: 0.6712180320762896\n",
      "\n",
      "Modelo de Naive Bayes entrenado en el pliegue 6:\n",
      "Precisión: 0.6654746423927178\n",
      "\n",
      "Modelo de Naive Bayes entrenado en el pliegue 7:\n",
      "Precisión: 0.6702427394885132\n",
      "\n",
      "Modelo de Naive Bayes entrenado en el pliegue 8:\n",
      "Precisión: 0.6741439098396186\n",
      "\n",
      "Modelo de Naive Bayes entrenado en el pliegue 9:\n",
      "Precisión: 0.6736020806241872\n",
      "\n",
      "Modelo de Naive Bayes entrenado en el pliegue 10:\n",
      "Precisión: 0.6675335934113568\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo de Naive Bayes.\n",
    "naive_bayes_model = GaussianNB()\n",
    "\n",
    "# Crear un pipeline con escalado de características\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Escalar las características\n",
    "    ('model', naive_bayes_model)        # Modelo de regresión logística\n",
    "])\n",
    "\n",
    "# Obtener todos los modelos de Naive Bayes entrenados durante la validación cruzada\n",
    "naive_bayes_models = [clone(pipeline).fit(X_train.iloc[train_idx], y_train.iloc[train_idx]) for train_idx, _ in metodo_control_cv.split(X_train, y_train)]\n",
    "\n",
    "# Imprimir información sobre el modelo GaussianNB.\n",
    "for i, modelo in enumerate(naive_bayes_models):\n",
    "    print(f\"\\nModelo de Naive Bayes entrenado en el pliegue {i + 1}:\")\n",
    "    # Printing accuracy or other performance metrics\n",
    "    accuracy = modelo.score(X_train, y_train)\n",
    "    print(\"Precisión:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Apartado 7\n",
    "Análisis comparativo de todos los modelos en train y en validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
